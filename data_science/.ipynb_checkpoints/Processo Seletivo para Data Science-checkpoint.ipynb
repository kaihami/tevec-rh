{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Olá Candidato\n",
    "\n",
    "Esse é o processo seletivo para a equipe de Data Science da Tevec. Ficamos muito felizes com sua participação. Procuramos montar um processo interessante e que estimule a sua criatividade. \n",
    "\n",
    "Você será avaliado segundo os seguintes critérios:\n",
    "1. Raciocínio e forma de estruturação de problemas\n",
    "2. Qualidade e limpeza do código\n",
    "3. Legibilidade do seu código\n",
    "4. Documentação do código\n",
    "5. Domínio dos conceitos de *machine learning*\n",
    "6. Nos cases práticos, você será avaliado pela qualidade e criatividade das visualizações de dados propostas e pelos insights comentados dos cases\n",
    "7. Nos cases práticos, você será avaliado pela qualidade do seu código, capacidade de reprodução e tratamento de excessões\n",
    "8. A criatividade na criação e proposição de features contará como um diferencial\n",
    "\n",
    "Responda a cada pergunta entre os markdowns respectivos. Algumas perguntas requerem que a entrega seja feita por meio de um arquivo .py que será executado para avaliação.\n",
    "\n",
    "Boa sorte!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Parte 1] Perguntas Dissertativas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.\tSe você tivesse que propor uma aplicação para um produto de IA para o varejo brasileiro, qual seria e por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seria um produto para otimizar as vendas a partir do comportamento do usuário.\n",
    "\n",
    "Por exemplo, identificando quais as rotas que cada cliente faz na loja e se há algum padrão referente ao tempo parado na frente de um produto e as compras que esse cliente realiza.\n",
    "\n",
    "Se esse cliente apresenta uma maior tendência de efetuar novas compras de acordo com a disposição dos produtos/promoções.\n",
    "\n",
    "A entrada seria dados de video (para acompanhar e entender o padrão de movimentação do cliente), \n",
    "que poderia gerar por exemplo quais caminhos os clientes costumam seguir, tempo parado em determinados pontos,\n",
    "quais produtos foram comprados e em qual ordem.\n",
    "\n",
    "Como saída pode ser verificado qual o comportamento do cliente de acordo com a disposição dos produtos\n",
    "se promoções induzem um certo comportamento e como o conjunto de promoções modifica esse padrão.  \n",
    "\n",
    "O reconhecimento desses padrões é fundamental para a previsibilidade do estoque/vendas, \n",
    "além de buscar aumentar a satisfação do cliente (favorecendo futuras compras), e compreender padrões que induzem a maior compra pelo consumidor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Qual sua experiência com metodologias ágeis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a minha experiência em pesquisa e desenvolvimento, os métodos ágeis fazem parte do meu dia a dia há mais de 3 anos.\n",
    "\n",
    "Acredito que a metodologia ágil permite identificar erros em uma fase precoce, evitando o desperdício de tempo e dinheiro, além de garantir que o foco dos esforços do time e para atender as necessidades do cliente. As reuniões entre os membros de um time multidisciplinar permitem a sinergia de trabalho entre os integrantes evitando conflitos, como por exemplo duas pessoas resolverem o mesmo problema, o rápido desenvolvimento de um protótipo inicial permitindo a antecipação da entrega de valor agregado ao cliente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.\tVocê já trabalhou em uma start-up? Se sim, comente sobre pontos positivos e negativos. Se não, nos conte suas expectativas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não tive a oportunidade de trabalhar diretamente com uma startup, porém em 2017 tive a oportunidade de estagiar na Harvard Medical School (Boston, USA) em que consegui colaborar em um projeto vinculado a uma startup. Essa experiência foi interessante para compreender as expectativas geradas sobre uma startup. \n",
    "\n",
    "Acredito que uma startup é um local de pesquisa e desenvolvimento continuo, em que a busca pela inovação faz parte do dia-a-dia. Pela minha experiência em pesquisa, uma característica presente na minha formação em que a resolução de problemas simples ou complexos fazem parte do meu cotidiano sempre buscando a entrega do melhor resultado em um curto período de tempo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Qual o conceito de machine learning mais recente que você aprendeu? O que lhe chamou atenção nesse conceito?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O conceito de ML que aprendi recentemente foi de redes neurais convolucionais (CNN). Na época um artigo conseguiu abstrair a sequência de DNA em uma imagem (transformando as bases do DNA (A, C, T, G) em um one-hot encoding). Com base nesse achado, comecei a aplicar essa mesma metodologia para uma sequência de proteína (cujo alfabeto apresenta 20 letras). Na área seria um grande avanço para detecção de proteínas homólogas (proteínas semelhantes entre si), em que o padrão ouro é a utilização de profile HMMs. Esse avanço foi alcançado recentemente pela Google AI liderado pela Lucy J. Cowell (https://doi.org/10.1101/626507), em que padrões que o profile HMM foi capaz de abstrair o problema conseguindo encontrar padrões em famílias que permaneciam desconhecidos. Antes desse artigo da Dra. Cowell ser publicado, eu consegui grandes avanços utilizando CNNs para compreender quando uma proteína é capaz de interagir com outra, mesmo que elas sejam iguais na percepção humana. \n",
    "O que mais me chamou a atenção nas CNNs foi o fato de que essas redes apresentam, em geral, um desempenho superior as redes neurais profundas e a arquiteturas utilizando-se somente LSTM, caso o problema seja possível de se abstrair em uma “imagem”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Comente algum artigo de Data Science que você tenha lido nas últimas semanas? O que lhe chamou atenção?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos artigos que me chamou atenção recentemente foi o intitulado “Self-tuning spectral clustering” Zelnik-Manor e Perona (2005). A formação de grupos a partir de indivíduos (clustering) é um problema clássico nas técnicas de reconhecimento de padrões. Um dos grandes problemas nas técnicas de clustering não supervisionado é a determinação do número de grupos, o k do algoritmo kNN. Apesar desse artigo apresentar mais de 10 anos após a sua publicação, o algoritmo desenvolvido por eles tornou-se um ótimo procedimento para encontrar o número ótimo de grupo automaticamente para a técnica de Spectral Clustering. Quase eliminando a análise manual, ou técnicas imprecisas como o método do cotovelo (elbow method), BIC (Bayesian Information Criterion) e AIC (Akaike Information Criterion), apesar de AIC/BIC apresentarem uma performance extremamente interessante em alguns casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Qual seu algoritmo preferido para iniciar a investigação de problemas de classificação e/ou regressão? Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O meu algoritmo para problemas de classificação e regressão com certeza é o Random Forest (RF). Esse algoritmo apresenta uma performance extremamente alta para muitos problemas e foi um dos principais algoritmos utilizado na área biológica, até a introdução de redes neurais mais complexas como as redes neurais convolucionais (CNNs) e a LSTM. As RFs eram escolhidas principalmente pelo baixo número de hiperparâmetros a serem otimizados, a pouca probabilidade de apresentar overfitting, a possibilidade de facilmente identificar as features com uma alta taxa de informação, e a sua simplicidade de interpretação. Apesar das redes neurais profundas (somente com camadas densas) poderem se assemelhar a uma RF ao adicionar mais neurônios e/ou camadas na sua arquitetura, essas redes neurais podem sofrer da maldição da dimensionalidade (curse of dimensionality) em que o mesmo não ocorre uma RF. A baixa complexidade das RFs permite a transposição de um modelo que antes era classificatório para um problema de regressão uma característica marcante desse algoritmo sendo um dos poucos, se não o único, em que esse procedimento é possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Explique para um leigo, como funciona um classificador RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo de Random Forest (RF), é um dos algoritmos mais simples de compreender o seu funcionamento. Uma forma de explicar o procedimento das RF é por meio de uma árvore de decisão. \n",
    "\n",
    "Imagine que você gostaria de viajar, mas não sabe para onde ir. Você pergunta para um amigo, vamos chama-lo de Pedro, “você pode me indicar um lugar para eu passar as férias?”. Pedro por sua vez começa a fazer diversas perguntas com resposta binária (somente duas opções), por exemplo:\n",
    "\n",
    "1-) Você gosta de frio ou de calor?\n",
    "2-) Você quer ir para o exterior ou não?\n",
    "3-) Você gosta de praia ou montanha?\n",
    "4-) Vai sozinho ou acompanhado?\n",
    "\n",
    "E assim por diante. A partir das experiências dele, ele irá te indicar um local. Essa seria uma árvore de decisão. As RFs por sua vez são compostas por várias árvores de decisões, para formar essas outras árvores Pedro te introduz um amigo dele, Thiago, que faz uma nova série de perguntas que podem ser iguais ou ligeiramente diferente. Como por exemplo adicionando a seguinte pergunta.\n",
    "\n",
    "5-) Você gosta de esportes radicais?\n",
    "\n",
    "E a partir dessas perguntas Thiago sugere um local para você, mas esse local não é igual ao sugerido pelo Pedro. Essa é a primeira característica das RFs, elas apresentam um baixo viés (bias), ao consultar mais de um amigo você tem diversas opiniões diminuindo a conclusão por apenas uma única resposta, por outro lado você aumenta a variância, no nosso caso temos dois locais distintos, logo qual escolher?\n",
    "\n",
    "Para escolher o local mais indicado Thiago apresenta um novo amigo, e esse por sua vez apresenta um novo e assim por diante até você ter uma boa quantidade de opiniões. Com isso a nossa única árvore de decisão, que era o nosso amigo Pedro, agora é uma grande floresta povoada centenas ou milhares de árvores de decisões. O local indicado em uma RF clássica é basicamente escolher o local mais votado, como por exemplo, o governador escolhido por um estado é uma escolha entre dois candidatos (no segundo turno das eleições) e apesar das outras opiniões serem importantes elas podem ser ignoradas, fornecendo apenas um resultado, ou serem consideradas permitindo fornecer uma probabilidade do por que o local escolhido foi esse e não outro. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Explique para um leigo, como funciona um regressor K-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine que nós temos muitas casas/prédios, como no caso de São Paulo. O valor de uma casa na região da Vila Nova Conceição (uma das regiões mais cara de São Paulo) difere de qualquer outro bairro como por exemplo o Jaçanã. Além do bairro outros aspectos influenciam o preço do imóvel, como por exemplo número de quartos, área útil, vagas de garagem e assim por diante. Logo um apartamento com 1 quarto e 25 metros quadrados não terá o mesmo preço de um apartamento de 200 metros quadrados na mesma região. Porém esses aspectos são importantes para determinar o seu preço.\n",
    "\n",
    "Nesse exemplo, fica claro como o kNN (k vizinhos mais próximos, em português) funciona, tanto a localização como as características do imóvel determinam o seu preço e esses podem ser utilizados para determinar o preço de um novo imóvel.\n",
    "\n",
    "Por exemplo, imagine que você irá construir duas casas para vender, uma na região do Jaçanã e outra na Vila Nova Conceição, ambas com as mesmas características, como número de quartos, tamanho, se tem piscina e etc. Agora você precisa colocar um preço nessas casas, apesar de ambas terem as mesmas especificações o preço das duas não será igual. Uma forma de determinar o preço é a partir das casas vizinhas a nova casa.\n",
    "\n",
    "Vamos pegar somente o caso da Vila Nova Conceição, pois o conceito se aplicar a casa no Jaçanã. Essa casa possui 200 metros quadrados, e as outras características estamos ignorando para simplicidade. Os vizinhos dessa casa possuem 180 metros, 220 metros quadrados e 50 metros quadrados. Por intuição o melhor seria pegar tanto o valor da casa de 180 e 220 metros quadrados e ignorar o valor da casa de 50 metros quadrados, já que é um tamanho muito diferente da casa recém construída. O valor da casa de 180 metros quadrados é de 1.8 milhão de reais, e da casa de 220 metros quadrados é de 2.2 milhão de reais, logo o preço da nova casa nessa região seria de 2.0 milhão de reais, a média dos valores das casas escolhidas, e pronto o nosso regressor kNN está pronto.\n",
    "\n",
    "No caso dos regressores kNN o número k é o número de vizinhos a serem considerados para estimar o valor (nesse caso k=2, se o k fosse 3 a casa de 50 metros quadrados entraria no nosso calculo final), os dados que definem qual vizinho será escolhido é baseado em diversas features (características), nesse caso as features são tanto a localização como o tamanho da casa. Baseado nessas características nós escolhemos as casas que mais se assemelham a nossa, depois de escolher os melhores (ou melhores vizinhos), basta fazer uma média do valor alvo (no caso o preço da casa) e assim descobrir o valor da nova casa.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Parte 2] Cases práticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: Classificação binária\n",
    "\n",
    "Considere os dados do arquivo artificial_binary_classification_data.csv. São dados gerados artificialmente para um problema de classificação binária. O Dataset contém features relevantes e irrelevantes. As features não são linearmente independentes (existem combinações lineares presentes no dataset). \n",
    "\n",
    "Faça uma análise descritiva das features e monte um modelo de classificação com uma técnica de ensemble da sua preferência (comente o motivo da escolha do algoritmo, suas vantagens e desvantagens).\n",
    "\n",
    "Esse case pode ser desenvolvido nesse notebook ou em outro, conforme sua preferência."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta no notebook case1 em anexo ([clique aqui](case1.ipynb))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: Séries temporais e modularização\n",
    "\n",
    "Considere os dados do conjunto \"serie_vendas.csv\". Com base nos dados:\n",
    "1. Faça uma descrição dos dados de acordo com análises que julgar relevantes, por exemplo: estatística descritiva, análise de sazonalidade, etc. Essas análises devem conter gráficos, visualizações e explicações.\n",
    "2. Qual o tipo de produto que você imagina que vende nessa loja? Justifique sua resposta.\n",
    "3. Liste os 10 SKUs que tiveram as maiores vendas no segundo semestre de 2017.\n",
    "4. Verifique a consistência entre a venda e o estoque do SKU que apresentou a maior venda no segundo semestre de 2017.\n",
    "5. Construa um modelo genérico de previsão de vendas de um SKU. Na entrada do modelo podem ser colocadas quaisquer variáveis, e a saída do modelo deve ser a previsão do volume total de vendas de um determinado SKU por dia.\n",
    "6. Modularize seu modelo do item (5) em uma classe que funciona como um serviço. o serviço deve esperar como entrada um arquivo json no formado serie_temporal_input.json e deve retornar como output um arquivo json no formato serie_temporal_output.json\n",
    "\n",
    "Os itens 1 a 5 podem ser desenvolvidos no próprio notebook. \n",
    "\n",
    "No caso do item 6, fique a vontade para construir suas análises e protópos no próprio notebook, mas a entrega deve ser realizada por meio de um arquivo .py conforme o exemplo serie_temporal.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request': {'dates': ['2018-01-01', '2018-01-02', '2018-01-03']}}\n",
      "{'response': [{'date': '2018-01-01', 'forecast': 10}, {'date': '2018-01-02', 'forecast': 15}, {'date': '2018-01-03', 'forecast': 20}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "i  = open(\"serie_vendas_input.json\")\n",
    "print(json.load(i))\n",
    "o = open(\"serie_vendas_output.json\")\n",
    "print(json.load(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As respostas referente aos itens 1-5 encontram-se no [notebook Case2 em anexo](Case2.ipynb)**  \n",
    "\n",
    "**O item 6 eu utilizei o modelo construído no item 5 porém modifiquei um pouco a estrutura do problema para ser facilmente utilizado na linha de comando.**\n",
    "\n",
    "**Foi adicionado opções de linha de comando, como por exemplo o arquivo json a ser lido (-i/--input), além de opções de output e formatação (-of/--output_format). Além do formato json (default) adicionei a opção de imprimir na tela um formato tabular(-of table), contendo as mesmas informações. Caso a opção seja no formato json, ele pode sair na saída padrão (standard output) ou para um arquivo (-f/--file [filename]). Para uma melhor compreensão desses argumentos foi adicionado um helper (-h/--help).**  \n",
    "**A resposta referente ao item 6 encontra-se no arquivo [serie_temporal.py](serie_temporal.py)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: Caixa Eletrônico\n",
    "\n",
    "Você foi encarregado de programar o software de um caixa eletrônico que trabalha com notas de 20 e 50. O caixa deve receber como input uma quantia e deve retornar a **quantidade mínima** de cédulas que atende a quantidade solicitada. \n",
    "\n",
    "O padrão de inputs é exemplificado no arquivo caixa_eletronico_input.json.\n",
    "\n",
    "O padrão de outputs bem sucedidos é exemplificado no arquivo caixa_eletronico_output_success.json. O padrão de outputs mal sucedidos é exemplificado no arquivo caixa_eletronico_output_fail.json.\n",
    "\n",
    "O arquivo caixa_eletronico_amostras_teste.json será utilizado para avaliar seu programa. \n",
    "\n",
    "Fique a vontade para construir suas análises e protópos no próprio notebook, mas a entrega deve ser realizada por meio de um arquivo .py conforme o exemplo caixa_eletronico.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'request': {'userID': 837495729487834, 'request': 120}}\n",
      "{'requester': {'userID': 837495729487834, 'requested': 120}, 'response': {'20': 1, '50': 2}}\n",
      "{'requester': {'userID': 837495729487834, 'requested': 120}, 'response': {'error': 'Sua mensagem de erro'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "i  = open(\"caixa_eletronico_input.json\")\n",
    "print(json.load(i))\n",
    "o_success = open(\"caixa_eletronico_output_success.json\")\n",
    "print(json.load(o_success))\n",
    "o_fail = open(\"caixa_eletronico_output_fail.json\")\n",
    "print(json.load(o_fail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta no arquivo em anexo ([caixa_eletronico.py](caixa_eletronico.py))**\n",
    "\n",
    "**Adicionalmente foi criado um arquivo json (caixa_eletronico_amostras_teste_add.json) que contêm um caso com erro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4: Deep Learning\n",
    "\n",
    "Construa uma rede neural para um problema de NLP para análise de sentimentos (sentiment analysis). O modelo deve ser construído utilizando um dos seguintes frameworks:\n",
    "\n",
    "- pytorch\n",
    "- tensorflow\n",
    "- keras\n",
    "\n",
    "O problema possui as seguintes especificações:\n",
    "\n",
    "1. A entrada são frases de 100 caracteres\n",
    "2. A saída são valores no intervalo (-1, 1) indicando se o sentimento é negativo, neutro ou positivo\n",
    "\n",
    "Explique a estrutura e motivo da arquitetura escolhida.\n",
    "\n",
    "A entrega desse caso pode ser realizada no próprio notebook, ou num arquivo .py se preferir. A avaliação do case será baseada na estrutura da rede e nos comentários da estrutura. Coloque as referências bibliográficas que achar relevantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eu resolvi desenvolver esse case utilizando uma rede neural convolucional (CNN) seguida de uma rede neural densa clássica. A ideia de utilizar essa arquitetura foi baseada em um artigo recente em que foi utilizado o mesmo conjunto de dados ([Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis](https://ieeexplore.ieee.org/document/8249013))**  \n",
    "\n",
    "**Nesse artigo a arquitetura era composta por diversas CNNs + LSTM, e foi obtido uma acurácia de cerca de 90%, um nível extremamente alto para análise de sentimento. Por motivos de tempo, eu escolhi aplicar o mesmo conceito utilizando-se uma CNN tradicional seguida de uma rede densa. Utilizando-se essa arquitetura obtive uma acurácia de 81%, no conjunto de teste, o qual pode ser melhorado utilizando-se mais camadas convolucionais, ou mesmo seguir a arquitetura proposta pelo artigo, reproduzindo os dados obtidos.**\n",
    "\n",
    "**Adicionalmente ao modelo, eu criei em um programa (sentimental_analysis.py) com uma interface de linha de comando. Um arquivo teste também se encontra no mesmo diretório (sentimental_analysis.ipt). A entrada desse programa é uma frase digitada na linha de comando (./sentimental_analysis.py ‘I loved this movie so much’) ou um arquivo texto em que cada linha é uma frase (como no arquivo sentimental_analysis.ipt), a análise utiliza o modelo já previamente treinado (sentiment_analysis.h5) e a saída é redirecionada para a saída padrão (standard output) no formato (Score “\\t” Phrase).**\n",
    "\n",
    "**A arquitetura, treinamento e alguns “silly-tests” podem ser encontrados [nesse notebook (Case4)](Case4.ipynb)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
